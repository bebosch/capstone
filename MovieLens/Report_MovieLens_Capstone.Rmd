---
title: "Report on MovieLens Project"
author: "Benedikt Bosch"
date: "26th July 2020"
output: pdf_document
---

```{r setup, include=FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = 'center')
```

# Overview over the MovieLens project

For this capstone project within Harvard's Professional Certificate in Data Science, I created a movie recommendation system using the MovieLens dataset and all the tools shown throughout the courses in this certificate. Within this project, I trained a machine learning algorithm using a training subset of the MovieLens dataset in order to predict the movie ratings in the complementary validation set. Doing so, my machine learning algorithm was able to yield a **RMSE** (residual mean squared error) of **0.86413** by using a **regularized movie + user + time + genre + publish effect model**.

# MovieLens dataset

The original MovieLens dataset was generated by the GroupLens research lab. It includes over 20 million ratings for over 27,000 movies by more than 138,000 users. Within this project, I used a subset of the MovieLens dataset with 10 million ratings and 100,000 tag applications applied to 10,000 movies by 72,000 users. This dataset will be referred to in the following as MovieLens datset or as original datset. It can be found under the following link: https://grouplens.org/datasets/movielens/10m/

# Approach and analysis overview

I developed my algorithm using the edx test set (90% of the data of the MovieLens dataset). For the final test of the algorithm, I predicted movie ratings in the validation set (10% of the data of the MovieLens dataset) as if they were unknown. To evaluate how close my predictions were to the true values in the validation set, RMSE (residual mean squared error) was used as loss function.

` `

\begin{center}
$\text{RMSE} = \displaystyle \frac{\sqrt{\sum_{t=1}^{T}(\hat{y_{t}}-y_{t})^{2}}}{T}$
\end{center}

` `

```{r RMSE, echo = FALSE, message = FALSE}
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings) ^ 2))
}
```

Within this function, $\hat{y_{t}}$ represents the prediction for each rating $t$ and $y_{t}$ the true value of the respective rating within the validation set.

\newpage

# Data cleaning and creation of train and validation sets

```{r libraries, echo = FALSE, message = FALSE}
# install all required libraries (note: this process could take a couple of minutes)
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(knitr)) install.packages("data.table", repos = "http://cran.us.r-project.org")

# load all required libraries
library(tidyverse)
library(caret)
library(data.table)
library(lubridate)
library(knitr)

# set digit places to 5
options(digits = 5)
```

```{r download_data, echo = FALSE, message = FALSE}
# download metadata
dl <- tempfile()
url = "http://files.grouplens.org/datasets/movielens/ml-10m.zip"
download.file(url, dl)
ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))
movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")

# convert downloaded metadata to data frame
movies <- as.data.frame(movies) %>% 
  mutate(movieId = as.numeric(levels(movieId))[movieId],
         title = as.character(title),
         genres = as.character(genres))
```

In a first step, after downloading the raw data, I inspected the head of the movie raw data (without ratings).

```{r movies_head, results = 'asis'}
kable(movies[1:6, ])
```

The excerpt of the raw data revealed that the data was not tidy as the movie title and the year the movie was published are concatenated. It was thus necessary to split the column values into two seperate pieces of information. Even if some movies from different years had the same name, they could still be uniquely identified by the movieId.

```{r data_cleaning, echo = FALSE, message = FALSE}
# split movie title and year
movies <- movies %>% 
  extract(title, c("title", "year"), regex = "([A-Za-z\\,\\s]*)\\s(\\(\\d{4}\\))") %>% 
  mutate(year = str_replace_all(year, "[\\(\\)]", ""))

# combine movie metadata and user ratings
movielens <- left_join(ratings, movies, by = "movieId")
```

After splitting the movie title and the year of publication, I combined the movie raw data with the user ratings to create the MovieLens dataset. The head of the dataset looks as following.

**Remark:** *The year column is in character format and will be converted to date format for visualization purposes at a later stage.*

```{r movielens_head, results = 'asis'}
kable(movielens[1:6, ])
```

As the timestamp column was not readable, it needed to be converted into the ISO 8601 format. For calculation purposes, this new date column is rounded by week.

```{r convert_timestamp, echo = FALSE, message = FALSE}
movielens <- movielens %>% 
  mutate(date = as_datetime(timestamp)) %>% 
  mutate(date = round_date(date, unit = "week")) %>% 
  select(-timestamp)
```

Another look at the head of the cleaned MovieLens dataset reveals that the data is tidy now.

```{r movielens_head_2, results = 'asis'}
kable(movielens[1:6, ])
```

\newpage

Before exploring the data in more detail and training the machine learning algorithm, a train and test set needed to be created from the MovieLens data. The validation set contained 10% of the MovieLens data and was left untouched until the model was finalized. It was only used to compute the final RMSE. The train set (called edx set) was split into a "real" train set (90% of the edx set) and a test set to assess how close my rating predictions with different models were to the true values.

```{r train_and_test_set, echo = FALSE, message = FALSE, warning = FALSE}
# create edx and validation set (10% of movielens data)
set.seed(1, sample.kind="Rounding")
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

# create train and test set within edx set
edx_test_index <- createDataPartition(edx$rating, times = 1, p = 0.1, list = FALSE)
train_set <- edx[-edx_test_index,]
test_set <- edx[edx_test_index,]
test_set <- test_set %>% 
  semi_join(train_set, by = "movieId") %>%
  semi_join(train_set, by = "userId")
rm(dl, ratings, movies, test_index, temp, removed)
```

After cleaning the data and creating the train, test and validation set, the three datsets comprised the follwing number of ratings:

```{r overview_datasets, echo = FALSE}
overview_datasets <- tibble(Dataset = c("Train set", "Test set", "Validation set"), 
           Ratings = c(nrow(train_set), nrow(test_set), nrow(validation)))
kable(overview_datasets)
```

# Insights gained through data exploration and visualization

In a first step, I had a look at the total number of unique users, the total number of unique movies and the average rating in the MovieLens dataset.

```{r data_summary, message = FALSE, comment = ""}
unique_users = length(unique(movielens$movieId))
unique_movies = length(unique(movielens$userId))
average_rating = mean(movielens$rating)
```

The number of unique users is `r unique_users`, the number of unique users is `r unique_movies` and the average rating is `r average_rating`.

As we learned in class about biases, I wanted to get a better understanding of potential noisy factors that influence movie ratings. By looking at the column names of the MovieLens dataset, I could identify five potential factors that influence movie ratings: the user, the movie, the year the movie was published, the movie genre(s) and the week the movie was rated. Thus, I decided to inspect the rating distribution of those five factors in more detail.

For every factor, I grouped its elements and calculated the average rating for each of the groups. Afterwards, I visualized the results to get a better understanding of the distribution and included the "averaged average" (red line). In the following, the results of this data exploration are visualized.

` `

```{r movie_exploration, echo = FALSE, message = FALSE}
# Average rating per movie
movie_mu <- movielens %>%
  group_by(movieId) %>% 
  summarize(b_i = mean(rating)) %>%
  summarize(movie_mu = mean(b_i)) %>%
  .$movie_mu
movielens %>% 
  group_by(movieId) %>% 
  summarize(b_i = mean(rating)) %>%
  ggplot(aes(b_i)) + 
  geom_histogram(bins = 30, color = "black") + 
  geom_vline(xintercept = movie_mu, color = "red") +
  ggtitle("Distribution of average rating per movie") +
  theme(plot.title = element_text(hjust = 0.5), text = element_text(family = "Times")) +
  labs(x = "Average rating per movie", y = "Number of movies")
```

```{r user_exploration, echo = FALSE, message = FALSE}
# Average rating per user
user_mu <- movielens %>%
  group_by(userId) %>% 
  summarize(b_u = mean(rating)) %>%
  summarize(user_mu = mean(b_u)) %>%
  .$user_mu
movielens %>%
  group_by(userId) %>% 
  summarize(b_u = mean(rating)) %>% 
  ggplot(aes(b_u)) +
  geom_histogram(bins = 30, color = "black") +
  geom_vline(xintercept = user_mu, color = "red") +
  ggtitle("Distribution of average rating per user") +
  theme(plot.title = element_text(hjust = 0.5), text = element_text(family = "Times")) +
  labs(x = "Average rating per user", y = "Number of users")
```

```{r time_exploration, echo = FALSE, message = FALSE}
# Average rating across time
time_mu <- movielens %>%
  group_by(date) %>% 
  summarize(b_t = mean(rating)) %>%
  summarize(time_mu = mean(b_t)) %>%
  .$time_mu
movielens %>%
  group_by(date) %>%
  summarize(b_t = mean(rating)) %>%
  ggplot(aes(date, b_t)) +
  geom_point(alpha = 0.2) +
  geom_smooth() +
  geom_hline(yintercept = time_mu, color = "red") +
  ggtitle("Distribution of average rating across time") +
  theme(plot.title = element_text(hjust = 0.5), text = element_text(family = "Times")) +
  labs(x = "Year (in week intervals)", y = "Average rating per calendar week")
```

```{r genre_exploration, echo = FALSE, message = FALSE}
# Average rating per genre
genre_mu <- movielens %>%
  group_by(genres) %>% 
  summarize(b_g = mean(rating)) %>%
  summarize(genre_mu = mean(b_g)) %>%
  .$genre_mu
movielens %>%
  group_by(genres) %>% 
  summarize(b_g = mean(rating)) %>% 
  ggplot(aes(b_g)) +
  geom_histogram(bins = 30, color = "black") +
  geom_vline(xintercept = genre_mu, color = "red") +
  ggtitle("Distribution of average rating per genre") +
  theme(plot.title = element_text(hjust = 0.5), text = element_text(family = "Times")) +
  labs(x = "Average rating per genre", y = "Number of genres")
```

```{r publication_exploration, echo = FALSE, message = FALSE}
# Average rating across year of publication
publish_mu <- movielens %>%
  group_by(year) %>% 
  summarize(b_p = mean(rating)) %>%
  summarize(publish_mu = mean(b_p)) %>%
  .$publish_mu
movielens %>%
  mutate(year = as.Date(year, "%Y")) %>%
  group_by(year) %>%
  summarize(b_p = mean(rating)) %>%
  ggplot(aes(year, b_p)) +
  geom_point(alpha = 0.2) +
  geom_smooth() +
  geom_hline(yintercept = publish_mu, color = "red") +
  ggtitle("Distribution of average rating across year of publication") +
  theme(plot.title = element_text(hjust = 0.5), text = element_text(family = "Times")) +
  labs(x = "Year of publication", y = "Average rating per year of publication")
```

For all five factors, biases are clearly discernible: some movies are rated better than others, some users are more cranky than others, average ratings tended to decrease until 2005, some genres are more popular than others and old movies tend to better than new ones. Consequently, an appropriate machine learning algorithm needs to take into account all of those factors.

# Modeling approach

To model the movie recommendation system, I decided to start with the simplest possible recommendation system and adjusted it factor by factor, based on the insights described in the previous section. For the adjustment, I grouped each factor's elements and calculated each group's average rating. To receive the respective bias value, $b_{x}$, I took the average of the distance between the true rating in the train set and the predicted rating by the algorithm *(as demonstrated later in the code for the movie + user + time + genre + publish effect model)*. The formula of the resulting model is

` `

\begin{center}
$Y_{i,u,t,g,p} = \mu + b_{i} + b_{u} + b_{t} + b_{g} + b_{p} + \epsilon_{i,u,t,g,p}$
\end{center}

` `

with $Y_{i,u,t,g,p}$ respresenting user $u$'s rating at time $t$ for movie $i$ of genre $g$, published in year $p$.

Due to the large amount of unique users and unique movies, there were some movies that have just been rated by a very low number of users, bearing the risk that the predictions based on this data are significantly distorted. For this reason, I performed regularization in a last step to fine tune the model. Regularization limits the variability of effect sizes by penalizing large estimates from small sample sizes.

\newpage

In total, there were seven different models incorporating the different aspects of the modeling approach:

1. Average model
2. Movie effect model
3. Movie + user effect model
4. Movie + user + time effect model
5. Movie + user + time + genre effect model
6. Movie + user + time + genre + publish effect model
7. Regularized movie + user + time + genre + publish effect model

Each model was trained using the train set. After training the model, the ratings in the test set were predicted and the resulting RMSE calculated. Comparing the resulting RMSEs, it was apparent that the more effects were included, i.e. the more complex the model got through including noisy factors, the better the RMSE. As soon as the RMSE reached a level sufficient to my expectations, I applied the resulting model to the validation set to receive the final RMSE of my machine learning algorithm.

## Average model

The cornerstone of the algorithm is the average model. It is the simplest possible recommendation system and predicts the same rating for all movies, regardless of the movie, user, time of rating, genre or year of publication. The predicted rating is simply the average rating across all movies and all users in the train set.

```{r average_model, message = FALSE, warning = FALSE}
# average rating across all users
mu <- mean(train_set$rating)

# use mu to compute RMSE on the test set
mu_rmse <- RMSE(mu, test_set$rating)

# use mu_rmse to create rmse_results table
rmse_results <- data_frame(method = "Average Model", RMSE = mu_rmse)
```

The average model yields a RMSE of `r mu_rmse` and leaves significant room for improvement.

## Movie + user + time + genre + publish effect model

To improve the average model, I included the five different factors one after another. In this report, I only show the code for incorporating the last factor, the publish effect, into the model.

```{r movie_model, echo = FALSE, message = FALSE}
# calculate the movie effect
movie_avgs <- train_set %>% 
  group_by(movieId) %>% 
  summarize(b_i = mean(rating - mu))

# calculate predicted ratings when incorporating the movie effect
movie_effect_predicted_ratings <- mu + test_set %>% 
  left_join(movie_avgs, by = 'movieId') %>% 
  .$b_i

# use predicted ratings by incorporating movie effect to compute RMSE on the test set
movie_effect_rmse <- RMSE(test_set$rating, movie_effect_predicted_ratings)

# add movie_effect_rmse to rmse_results table
rmse_results <- bind_rows(rmse_results, data_frame(method = "Movie Effect Model", RMSE = movie_effect_rmse))
```

```{r user_model, echo = FALSE, message = FALSE}
# calculate the user effect
user_avgs <- train_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i))

# calculate predicted ratings when additionally incorporating the user effect
user_effect_predicted_ratings <- test_set %>% 
  left_join(movie_avgs, by = 'movieId') %>% 
  left_join(user_avgs, by = 'userId') %>% 
  mutate(pred = mu + b_i + b_u) %>% 
  .$pred

# use predicted ratings by incorporating user effect to compute RMSE on the test set
user_effect_rmse <- RMSE(test_set$rating, user_effect_predicted_ratings)

# add user_effect_rmse to rmse_results table
rmse_results <- bind_rows(rmse_results, data_frame(method = "Movie + User Effect Model", 
                                                   RMSE = user_effect_rmse))
```

```{r time_model, echo = FALSE, message = FALSE}
# calculate the time effect
week_avgs <- train_set %>% 
  left_join(movie_avgs, by = 'movieId') %>% 
  left_join(user_avgs, by = 'userId') %>%
  group_by(date) %>% 
  summarize(b_t = mean(rating - mu - b_i - b_u))

# calculate predicted ratings when additionally incorporating the time effect
time_effect_predicted_ratings <- test_set %>% 
  left_join(movie_avgs, by = 'movieId') %>% 
  left_join(user_avgs, by = 'userId') %>% 
  left_join(week_avgs, by = "date") %>%
  mutate(pred = mu + b_i + b_u + b_t) %>% 
  .$pred

# use predicted ratings by incorporating time effect to compute RMSE on the test set
time_effect_rmse <- RMSE(test_set$rating, time_effect_predicted_ratings)

# add time_effect_rmse to rmse_results table
rmse_results <- bind_rows(rmse_results, data_frame(method = "Movie + User + Time Effect Model", 
                                                   RMSE = time_effect_rmse))
```

```{r genre_model, echo = FALSE, message = FALSE}
# calculate the genre effect
genre_avgs <- train_set %>% 
  left_join(movie_avgs, by ='movieId') %>%
  left_join(user_avgs, by = "userId") %>% 
  left_join(week_avgs, by = "date") %>%
  group_by(genres) %>%
  summarize(b_g = mean(rating - mu - b_i - b_u - b_t))

# calculate predicted ratings when additionally incorporating the genre effect
genre_effect_predicted_ratings <- test_set %>% 
  left_join(movie_avgs, by = 'movieId') %>% 
  left_join(user_avgs, by = 'userId') %>% 
  left_join(week_avgs, by = "date") %>%
  left_join(genre_avgs, by = "genres") %>% 
  mutate(pred = mu + b_i + b_u + b_t + b_g) %>% 
  .$pred

# use predicted ratings by incorporating genre effect to compute RMSE on the test set
genre_effect_rmse <- RMSE(test_set$rating, genre_effect_predicted_ratings)

# add genre_effect_rmse to rmse_results table
rmse_results <- bind_rows(rmse_results, data_frame(method = "Movie + User + Time + Genre Effect Model", 
                                                   RMSE = genre_effect_rmse))
```

```{r publish_effect, message = FALSE}
# calculate publish effect
publish_avgs <- train_set %>% 
  left_join(movie_avgs, by ='movieId') %>%
  left_join(user_avgs, by = "userId") %>% 
  left_join(week_avgs, by = "date") %>%
  left_join(genre_avgs, by = "genres") %>% 
  group_by(year) %>%
  summarize(b_p = mean(rating - mu - b_i - b_u - b_t - b_g))

# calculate predicted ratings when additionally incorporating the genre effect
publish_effect_predicted_ratings <- test_set %>% 
  left_join(movie_avgs, by = 'movieId') %>% 
  left_join(user_avgs, by = 'userId') %>% 
  left_join(week_avgs, by = "date") %>%
  left_join(genre_avgs, by = "genres") %>% 
  left_join(publish_avgs, by = "year") %>%
  mutate(pred = mu + b_i + b_u + b_t + b_g + b_p) %>% 
  .$pred

# use predicted ratings by incorporating publish effect to compute RMSE on the test set
publish_effect_rmse <- RMSE(test_set$rating, publish_effect_predicted_ratings)
```

```{r publish_rmse_table, echo = FALSE, message = FALSE}
# add publish_effect_rmse to rmse_results table
rmse_results <- bind_rows(rmse_results, 
                          data_frame(method = "Movie + User + Time + Genre + Publish Effect Model", 
                                     RMSE = publish_effect_rmse))
```


After incorporating all effects into the model, the RMSE results table reveals that the algorithm was significantly improved compared to the initial average model.

```{r view_results_table, results = 'asis'}
kable(rmse_results)
```

## Regularized movie + user + time + genre + publish effect model

In a last step, I performed regularization as described in the modeling approach. Instead of minimizing the RMSE (error term $\epsilon$), I minimized the following equation (penalty term $\lambda$):

` `

\begin{center}
$\frac{1}{N}\sum_{u,i,t,g,p}^{}(Y_{i,u,t,g,p} - \mu - b_{i} - b_{u} - b_{t} - b_{g} - b_{p})^2 + \lambda(\sum_{i}{}b^{2}_{i} + \sum_{u}{}b^{2}_{u} + \sum_{t}{}b^{2}_{t} + \sum_{g}{}b^{2}_{g} + \sum_{p}{}b^{2}_{p})$
\end{center}

` `

```{r lambda_search, echo = FALSE, message = FALSE}
# find optimal lambda
lambdas <- seq(0, 10, 0.25)
rmses <- sapply(lambdas, function(l){
  b_i <- train_set %>%
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))
  b_u <- train_set %>% 
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+l))
  b_t <- train_set %>%
    left_join(b_i, by="movieId") %>%
    left_join(b_u, by="userId") %>%
    group_by(date) %>%
    summarize(b_t = sum(rating - b_u - b_i - mu)/(n()+l))
  b_g <- train_set %>%
    left_join(b_i, by="movieId") %>%
    left_join(b_u, by="userId") %>%
    left_join(b_t, by="date") %>%
    group_by(genres) %>%
    summarize(b_g = sum(rating - b_t - b_u - b_i - mu)/(n()+l))
  b_p <- train_set %>%
    left_join(b_i, by="movieId") %>%
    left_join(b_u, by="userId") %>%
    left_join(b_t, by="date") %>%
    left_join(b_g, by="genres") %>%
    group_by(year) %>%
    summarize(b_p = sum(rating - b_g - b_t - b_u - b_i - mu)/(n()+l))
  predicted_ratings <- test_set %>%
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    left_join(b_t, by = "date") %>%
    left_join(b_g, by = "genres") %>%
    left_join(b_p, by = "year") %>%
    mutate(pred = mu + b_i + b_u + b_t + b_g + b_p) %>%
    .$pred
  return(RMSE(test_set$rating, predicted_ratings))
})

# calculate optimal lambda
lambda <- lambdas[which.min(rmses)]
```

The optimal lambda that minimizes the penalty term and the equation can be identified graphically. The optimal value of lambda is `r lambda`.

```{r lambda_plot}
qplot(lambdas, rmses)
```

```{r regularization, echo = FALSE, message = FALSE}
# perform regularization with optimal lambda (equivalent to minmum of rmses)
regularized_rmse = min(rmses)

# add regularized_rmse to rmse_results table (= Regularized Movie + User + Time + Genre + Year Effect Model)  
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Regularized Movie + User + Time + Genre + Year Effect Model", 
                                     RMSE = regularized_rmse))
```

After performing regularization, I was able to further reduce the RMSE on the test set to `r regularized_rmse`. I considered this RMSE sufficient for a good and robust movie recommendation system and thus moved on to apply my movie recommendation system to the validation set.

# Results

```{r validation, message = FALSE}
# calculate different regularized effects on whole edx set (before only test set)
movie_reg_avgs <- edx %>% 
  group_by(movieId) %>% 
  summarize(b_i = sum(rating - mu)/(n()+lambda), n_i = n())
user_reg_avgs <- edx %>% 
  left_join(movie_reg_avgs, by="movieId") %>%
  group_by(userId) %>%
  summarize(b_u = sum(rating - b_i - mu)/(n()+lambda), n_u = n())
time_reg_avgs <- edx %>%
  left_join(movie_reg_avgs, by = 'movieId') %>%
  left_join(user_reg_avgs, by = 'userId') %>%
  group_by(date) %>%
  summarize(b_t = sum(rating - b_u - b_i - mu)/(n()+lambda), n_t = n())
genre_reg_avgs <- edx %>%
  left_join(movie_reg_avgs, by = 'movieId') %>%
  left_join(user_reg_avgs, by = 'userId') %>%
  left_join(time_reg_avgs, by = 'date') %>%
  group_by(genres) %>%
  summarize(b_g = sum(rating - b_t - b_u - b_i - mu)/(n()+lambda), n_g = n())
publish_reg_avgs <- edx %>%
  left_join(movie_reg_avgs, by = 'movieId') %>%
  left_join(user_reg_avgs, by = 'userId') %>%
  left_join(time_reg_avgs, by = 'date') %>%
  left_join(genre_reg_avgs, by = 'genres') %>%
  group_by(year) %>%
  summarize(b_p = sum(rating - b_g - b_t - b_u - b_i - mu)/(n()+lambda), n_p = n())

# use the regularized model to calculate predicted ratings on the validation set
validation_predicted_ratings <- validation %>%
  left_join(movie_reg_avgs, by='movieId') %>%
  left_join(user_reg_avgs, by = "userId") %>%
  left_join(time_reg_avgs, by = "date") %>%
  left_join(genre_reg_avgs, by = "genres") %>%
  left_join(publish_reg_avgs, by = "year") %>%
  mutate(pred = mu + b_i + b_u + b_t + b_g + b_p) %>%
  .$pred

# calculate final RMSE on validation set
validation_rmse <- RMSE(validation$rating, validation_predicted_ratings)
```

```{r validation_rmse_table, echo = FALSE, message = FALSE}
# add validation_rmse to rmse_results table (= Final RMSE on Validation Set)
rmse_results <- bind_rows(rmse_results,data_frame(method="Final RMSE on Validation Set",  
                                     RMSE = validation_rmse))
```

When applying my final algrorithm, the regularized model, on the validation set, I was able to achive a **RMSE** of **`r validation_rmse`**. This is the final RMSE of my movie recommendation system based on my own machine learning algorithm.

# Conclusion

```{r output, echo = FALSE, results = 'asis'}
kable(rmse_results)
```

By incorporating five different bias into my algorithm and by performing regularization, I was able to significantly reduce the RMSE of the initial average model by `r mu_rmse` to `r regularized_rmse` (RMSE of the regularized model on the test set). Applying the algorithm onto the validation set yielded a final RMSE of `r validation_rmse`.

In the future, I aim to further improve my movie recommendation system by modeling the time and publish effects with smooth functions. Furthermore, the genres of each movie could be treated in an improved fashion by grouping movies into similiar clusters (that share some but not all genres), not only identical clusters.
